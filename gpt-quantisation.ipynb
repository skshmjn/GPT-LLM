{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install auto-round\n!pip install auto-gptq accelerate datasets optimum\n!pip install -U bitsandbytes\n!pip install autoawq \n!pip install --upgrade torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from huggingface_hub import login\n# read_token = \"\"\n# write_token = ''\n# login(token = read_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:55:54.777563Z","iopub.execute_input":"2024-12-25T13:55:54.777887Z","iopub.status.idle":"2024-12-25T13:55:55.184701Z","shell.execute_reply.started":"2024-12-25T13:55:54.777864Z","shell.execute_reply":"2024-12-25T13:55:55.183768Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-3.2-1B\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:44:27.585579Z","iopub.execute_input":"2024-12-25T13:44:27.585908Z","iopub.status.idle":"2024-12-25T13:44:27.589679Z","shell.execute_reply.started":"2024-12-25T13:44:27.585880Z","shell.execute_reply":"2024-12-25T13:44:27.588650Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp_delme.p\")\n    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n    os.remove('temp_delme.p')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## AutoRound","metadata":{}},{"cell_type":"code","source":"from auto_round import AutoRound","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:58:13.165770Z","iopub.execute_input":"2024-12-25T13:58:13.166049Z","iopub.status.idle":"2024-12-25T13:58:13.170166Z","shell.execute_reply.started":"2024-12-25T13:58:13.166029Z","shell.execute_reply":"2024-12-25T13:58:13.169096Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model_for_auto_round = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)\ntokenizer_for_auto_round = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bits, group_size, sym = 4, 128, True\nautoround = AutoRound(model, tokenizer, bits=bits, group_size=group_size, batch_size=2, seqlen=512, sym=sym, gradient_accumulate_steps=4, device='cuda')\nautoround.quantize()\n\nautoround.save_quantized(model_name +\"-auto-round-4bit\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_for_auto_round.push_to_hub(model_name.split('/')[1] + \"-auto-round-4bit\", token=write_token)\ntokenizer_for_auto_round.push_to_hub(model_name.split('/')[1] + \"-auto-round-4bit\",token=write_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## AWQ","metadata":{}},{"cell_type":"code","source":"from awq import AutoAWQForCausalLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:44:23.834223Z","iopub.execute_input":"2024-12-25T13:44:23.834535Z","iopub.status.idle":"2024-12-25T13:44:23.838717Z","shell.execute_reply.started":"2024-12-25T13:44:23.834503Z","shell.execute_reply":"2024-12-25T13:44:23.837493Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\nmodel_for_awq = AutoAWQForCausalLM.from_pretrained(model_name, safetensors=True, device_map='cuda')\ntokenizer_for_awq = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:44:30.444171Z","iopub.execute_input":"2024-12-25T13:44:30.444518Z","iopub.status.idle":"2024-12-25T13:44:34.939754Z","shell.execute_reply.started":"2024-12-25T13:44:30.444489Z","shell.execute_reply":"2024-12-25T13:44:34.938813Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"905065e85a024311b1af68b5c8b5845c"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"model_for_awq.quantize(tokenizer_for_awq, quant_config=quant_config)\n\n# Save quantized model with safetensors\nmodel_for_awq.save_quantized(model_name +\"-awq-4-bit\", safetensors=True)\ntokenizer_for_awq.save_pretrained(model_name +\"-awq-4-bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:45:01.401447Z","iopub.execute_input":"2024-12-25T13:45:01.401750Z","iopub.status.idle":"2024-12-25T13:54:39.303305Z","shell.execute_reply.started":"2024-12-25T13:45:01.401729Z","shell.execute_reply":"2024-12-25T13:54:39.302530Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae928dde30854d4baf94ea2748b731af"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"val.jsonl.zst:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e9f34774344224823a5eedc0a08eb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd6e66c5f04046039aa0c1332c94cb4f"}},"metadata":{}},{"name":"stderr","text":"AWQ: 100%|██████████| 16/16 [09:15<00:00, 34.70s/it]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('Llama-3.2-1B-awq-4-bit/tokenizer_config.json',\n 'Llama-3.2-1B-awq-4-bit/special_tokens_map.json',\n 'Llama-3.2-1B-awq-4-bit/tokenizer.json')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"model_for_awq = AutoModelForCausalLM.from_pretrained(model_name + \"-awq-4-bit\", torch_dtype=torch.float16, device_map='auto')\ntokenizer_for_awq = AutoTokenizer.from_pretrained(model_name + \"-awq-4-bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:57:49.391065Z","iopub.execute_input":"2024-12-25T13:57:49.391379Z","iopub.status.idle":"2024-12-25T13:57:51.043904Z","shell.execute_reply.started":"2024-12-25T13:57:49.391355Z","shell.execute_reply":"2024-12-25T13:57:51.042962Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model_for_awq.push_to_hub(model_name.split('/')[1] +\"-awq-4-bit\", token=write_token)\ntokenizer_for_awq.push_to_hub(model_name.split('/')[1] +\"-awq-4-bit\",token=write_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:58:32.956250Z","iopub.execute_input":"2024-12-25T13:58:32.956581Z","iopub.status.idle":"2024-12-25T13:59:08.660416Z","shell.execute_reply.started":"2024-12-25T13:58:32.956556Z","shell.execute_reply":"2024-12-25T13:59:08.659667Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a40bd17ce40a483a966230879160c4f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"170398719f2b4afc84a9b39754f6d0eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5fde85df2e436ca80717fa2e10d22b"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/skshmjn/Llama-3.2-1B-awq-4-bit/commit/3e27b8ebdc9bcbf6cbf41e00fa4918e236fd141f', commit_message='Upload tokenizer', commit_description='', oid='3e27b8ebdc9bcbf6cbf41e00fa4918e236fd141f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/skshmjn/Llama-3.2-1B-awq-4-bit', endpoint='https://huggingface.co', repo_type='model', repo_id='skshmjn/Llama-3.2-1B-awq-4-bit'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## GPTQ","metadata":{}},{"cell_type":"code","source":"from optimum.gptq import GPTQQuantizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T14:01:41.200774Z","iopub.execute_input":"2024-12-25T14:01:41.201096Z","iopub.status.idle":"2024-12-25T14:01:41.401995Z","shell.execute_reply.started":"2024-12-25T14:01:41.201074Z","shell.execute_reply":"2024-12-25T14:01:41.401123Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, grad_output):\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd(cast_inputs=torch.float16)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model_for_gptq = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\ntokenizer_for_gptq = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T14:02:37.620870Z","iopub.execute_input":"2024-12-25T14:02:37.621164Z","iopub.status.idle":"2024-12-25T14:02:42.120636Z","shell.execute_reply.started":"2024-12-25T14:02:37.621141Z","shell.execute_reply":"2024-12-25T14:02:42.119955Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"quantizer = GPTQQuantizer(bits=4, dataset=\"c4\", model_seqlen = 2048)\nquantized_model = quantizer.quantize_model(model_for_gptq, tokenizer_for_gptq)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T14:03:14.729363Z","iopub.execute_input":"2024-12-25T14:03:14.729736Z","iopub.status.idle":"2024-12-25T14:14:40.779805Z","shell.execute_reply.started":"2024-12-25T14:03:14.729708Z","shell.execute_reply":"2024-12-25T14:14:40.778821Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/41.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3358fee9910642288e5116346e44c8d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"c4-train.00000-of-01024.json.gz:   0%|          | 0.00/319M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4f51cfef7f5474b964e1af9e625f825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"572a7d5a8b464bb7bdf3316106bfaed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing model.layers blocks :   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cbb5c7d6f3a45ca9408ff750968f145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Quantizing layers inside the block:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5055: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"quantized_model.save_pretrained(model_name + \"-GPTQ-4bit\", safetensors=True)\ntokenizer_for_gptq.save_pretrained(model_name + \"-GPTQ-4bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T14:14:40.781669Z","iopub.execute_input":"2024-12-25T14:14:40.781896Z","iopub.status.idle":"2024-12-25T14:14:43.144937Z","shell.execute_reply.started":"2024-12-25T14:14:40.781876Z","shell.execute_reply":"2024-12-25T14:14:43.144209Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('meta-llama/Llama-3.2-1B-GPTQ-4bit/tokenizer_config.json',\n 'meta-llama/Llama-3.2-1B-GPTQ-4bit/special_tokens_map.json',\n 'meta-llama/Llama-3.2-1B-GPTQ-4bit/tokenizer.json')"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"quantized_model.push_to_hub(model_name.split('/')[1] + \"-GPTQ-4bit\", token=write_token)\ntokenizer_for_gptq.push_to_hub(model_name.split('/')[1] + \"-GPTQ-4bit\",token=write_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T14:16:21.244678Z","iopub.execute_input":"2024-12-25T14:16:21.245004Z","iopub.status.idle":"2024-12-25T14:16:56.940094Z","shell.execute_reply.started":"2024-12-25T14:16:21.244977Z","shell.execute_reply":"2024-12-25T14:16:56.939385Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f8cbcb061f04208b45790a9ab48c5e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf4348a7fd2a4da9ab7621cff523d4c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7619aceaf7f4dea9205385c2d8c7420"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/skshmjn/Llama-3.2-1B-GPTQ-4bit/commit/a654ac6030376465d6ad4fe54b65fa58ac1c1e75', commit_message='Upload tokenizer', commit_description='', oid='a654ac6030376465d6ad4fe54b65fa58ac1c1e75', pr_url=None, repo_url=RepoUrl('https://huggingface.co/skshmjn/Llama-3.2-1B-GPTQ-4bit', endpoint='https://huggingface.co', repo_type='model', repo_id='skshmjn/Llama-3.2-1B-GPTQ-4bit'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}